defaults:
  - train-defaults 
  # here to declare start from pretrained model
  # - /models@model: silk-sfmlearner-sj
  - /models@model: silk-vgg

  ### Datasets
  
  # MOT
  - /datasets/raw_kitti_mot@loaders.training.dataset: training
  - /datasets/raw_kitti_mot@loaders.validation.dataset: validation
  
  # # SiT Full
  # - /datasets/SiT@loaders.training.dataset: training
  # - /datasets/SiT@loaders.validation.dataset: validation

  # # MOT
  # - /datasets/sfm_kitti_mot@loaders.training.dataset: training
  # - /datasets/sfm_kitti_mot@loaders.validation.dataset: validation

  ## pose formatted KITTI odom
  # - /datasets/pose_formatted_kitti_odom@loaders.training.dataset: training
  # - /datasets/pose_formatted_kitti_odom@loaders.validation.dataset: validation

  # ## SiT Outdoor Alley
  # - /datasets/SiT@loaders.training.dataset: training
  # - /datasets/SiT@loaders.validation.dataset: validation
  # ## formatted KITTI odom
  # - /datasets/formatted_kitti_odom@loaders.training.dataset: training
  # - /datasets/formatted_kitti_odom@loaders.validation.dataset: validation
  # ## KITTI raw
  # - /datasets/kitti_raw@loaders.training.dataset: training
  # - /datasets/kitti_raw@loaders.validation.dataset: validation
  # sfm KITTI raw
  # - /datasets/sfm_kitti_raw@loaders.training.dataset: training
  # - /datasets/sfm_kitti_raw@loaders.validation.dataset: validation
  ## COCO
  # - /datasets/coco@loaders.training.dataset: training
  # - /datasets/coco@loaders.validation.dataset: validation
  ## ImageNet
  # - /datasets/image-net@loaders.training.dataset: training
  # - /datasets/image-net@loaders.validation.dataset: validation
  ## MegaDepth
  # - /datasets/megadepth@loaders.training.dataset: train-val
  # - /datasets/megadepth@loaders.validation.dataset: test
  ## ScanNet (uncomment lines specified in this file)
  # - /datasets/scannet-frames@loaders.training.dataset: training-98-2-scan-split
  # - /datasets/scannet-frames@loaders.validation.dataset: validation-98-2-scan-split
  ## C + I + M + S
  # - /datasets/coco-image-net-megadepth-scannet@loaders.training.dataset: training
  # - /datasets/coco-image-net-megadepth-scannet@loaders.validation.dataset: validation
  ## C + I + M
  # - /datasets/coco-image-net-megadepth@loaders.training.dataset: training
  # - /datasets/coco-image-net-megadepth@loaders.validation.dataset: validation
  - _self_
  
trainer:
  max_epochs: 100
  limit_val_batches: 100
  limit_train_batches: 1000

  # log_every_n_steps: 10
  # max_epochs: 1000
  # limit_val_batches: 100
  # limit_train_batches: 1000

  # log_every_n_steps: 1
  # max_epochs: 1000
  # limit_val_batches: 1
  # limit_train_batches: 1


  # need two GPUs, PyTorch use GPU 1, and Jax uses GPU 0.
  gpus:
    - 1

  # fast_dev_run: true
  # profiler: simple
  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      # monitor: "val.f1"
      monitor: "val.total.loss"
      save_top_k: 10
      mode: "min"

loaders:
  training:
    batch_size: 1
    num_workers: 24
    collate_fn: ${mode.collate_fn}
    persistent_workers: falseSS
    ## Uncomment below for ScanNet only
    # dataset:
    #   extractor:
    #     - "color"
    #     - "room"
  validation:
    batch_size: 1
    num_workers: 24
    collate_fn: ${mode.collate_fn}
    persistent_workers: false
    ## Uncomment below for ScanNet only
    # dataset:
    #   extractor:
    #     - "color"
    #     - "room"

collate_fn:
  _target_: silk.transforms.tensor.AutoBatch
  transform:
    _target_: silk.transforms.abstract.Compose
    _args_:
      # convert tuples to named context
      - _target_: silk.transforms.abstract.Name
        _args_:
          - "image_1"
          - "image_2"
          - "rel_pose"
          - "intrinsics"
          - "depth_map_1"
          - "depth_map_2"
          # - null # we ignore the labels
      - _target_: silk.transforms.abstract.Map
        function:
          _target_: silk.transforms.abstract.Compose
          _args_:
            # convert PIL images to tensors
            - _target_: silk.transforms.tensor.ToTensor
            # convert image from HWC to CHW format
            - _target_: silk.transforms.cv.image.HWCToCHW
            # # crop sampling strategy #1
            # - _target_: torchvision.transforms.RandomResizedCrop
            #   _args_:
            #     - [164, 164] # size
            #     - [0.25, 1.0] # scale
            # crop sampling strategy #2
            - _target_: silk.transforms.cv.image.RandomCrop_getparam
              ## Sizes used for backbone ablation
              size: [164, 164] # VGG-4 (146 + 2x9)
              # size: [160, 160] # VGG-3 (146 + 2x7)
              # size: [156, 156] # VGG-2 (146 + 2x5)
              # size: [152, 152] # VGG-1 (146 + 2x3)
              # size: [324, 324] # ResFPN [LoFTR]
              # size: [332, 332] # PUNet [DISK]

              ## Sizes used for training image size ablation
              # size: [100, 100] # 82 + 2x9
              # size: [132, 132] # 114 + 2x9
              # size: [164, 164] # 146 + 2x9
              # size: [196, 196] # 178 + 2x9
              # size: [228, 228] # 210 + 2x9
              pad_if_needed: true
